#!/usr/bin/env python3
"""
ç®€åŒ–çš„ pre_task_for åŠŸèƒ½æµ‹è¯•è„šæœ¬
ä¸“æ³¨äºæµ‹è¯• pre_task_for é€»è¾‘ï¼Œè€Œä¸ä¾èµ–å®Œæ•´çš„ç¯å¢ƒ
"""

import sys
import os
from unittest.mock import Mock

# Add the src directory to the path
current_dir = os.path.dirname(os.path.abspath(__file__))
src_dir = os.path.dirname(current_dir)
if src_dir not in sys.path:
    sys.path.insert(0, src_dir)

from src.tasks.instance.campus_life_bench.task import CampusDatasetItem
from src.typings import SessionEvaluationOutcome


def test_campus_dataset_item_pre_task_for():
    """æµ‹è¯• CampusDatasetItem çš„ pre_task_for å­—æ®µ"""
    print("ğŸ“‹ æµ‹è¯•1: CampusDatasetItem pre_task_for å­—æ®µ")
    
    # æµ‹è¯•æœ‰ pre_task_for å­—æ®µçš„ä»»åŠ¡
    item_with_dependencies = CampusDatasetItem(
        task_id="prerequisite_task",
        task_type="email_sending",
        pre_task_for="dependent_task_1,dependent_task_2"
    )
    
    assert item_with_dependencies.pre_task_for == "dependent_task_1,dependent_task_2"
    print(f"âœ… å‰ç½®ä»»åŠ¡ {item_with_dependencies.task_id} è®¾ç½®äº†ä¾èµ–: {item_with_dependencies.pre_task_for}")
    
    # æµ‹è¯•æ²¡æœ‰ pre_task_for å­—æ®µçš„ä»»åŠ¡
    item_without_dependencies = CampusDatasetItem(
        task_id="independent_task",
        task_type="email_sending"
    )
    
    assert item_without_dependencies.pre_task_for is None
    print(f"âœ… ç‹¬ç«‹ä»»åŠ¡ {item_without_dependencies.task_id} æ²¡æœ‰ä¾èµ–")


def test_pre_task_for_parsing():
    """æµ‹è¯• pre_task_for å­—æ®µçš„è§£æé€»è¾‘"""
    print("\nğŸ“‹ æµ‹è¯•2: pre_task_for å­—æ®µè§£æ")
    
    test_cases = [
        ("task1,task2,task3", ["task1", "task2", "task3"]),
        ("task1, task2 , task3 ", ["task1", "task2", "task3"]),  # å¸¦ç©ºæ ¼
        ("task1,,task2", ["task1", "task2"]),  # ç©ºå€¼
        ("single_task", ["single_task"]),  # å•ä¸ªä»»åŠ¡
        ("", []),  # ç©ºå­—ç¬¦ä¸²
        ("   ", []),  # åªæœ‰ç©ºæ ¼
    ]
    
    for pre_task_for_str, expected in test_cases:
        # æ¨¡æ‹Ÿè§£æé€»è¾‘ï¼ˆä» _record_failed_prerequisite_task æ–¹æ³•ä¸­æå–ï¼‰
        if pre_task_for_str and pre_task_for_str.strip():
            actual = [task_id.strip() for task_id in pre_task_for_str.split(",") if task_id.strip()]
        else:
            actual = []
        
        assert actual == expected, f"è§£æ '{pre_task_for_str}' å¤±è´¥: æœŸæœ› {expected}, å®é™… {actual}"
        print(f"âœ… è§£æ '{pre_task_for_str}' -> {actual}")


def test_failed_prerequisite_tracking():
    """æµ‹è¯•å¤±è´¥å‰ç½®ä»»åŠ¡è·Ÿè¸ªé€»è¾‘"""
    print("\nğŸ“‹ æµ‹è¯•3: å¤±è´¥å‰ç½®ä»»åŠ¡è·Ÿè¸ª")
    
    # æ¨¡æ‹Ÿ CampusTask çš„ failed_prerequisite_tasks å­—å…¸
    failed_prerequisite_tasks = {}
    
    def mock_record_failed_prerequisite_task(session, current_item):
        """æ¨¡æ‹Ÿ _record_failed_prerequisite_task æ–¹æ³•"""
        if (session.evaluation_record.outcome == SessionEvaluationOutcome.INCORRECT and 
            current_item.pre_task_for is not None and 
            current_item.pre_task_for.strip() != ""):
            
            affected_task_ids = [task_id.strip() for task_id in current_item.pre_task_for.split(",") if task_id.strip()]
            
            if affected_task_ids:
                failed_prerequisite_tasks[current_item.task_id] = affected_task_ids
                print(f"ğŸ“ è®°å½•å¤±è´¥ä»»åŠ¡ {current_item.task_id} å½±å“: {affected_task_ids}")
    
    def mock_check_affected_by_failed_prerequisite(task_id):
        """æ¨¡æ‹Ÿ _check_affected_by_failed_prerequisite æ–¹æ³•"""
        for failed_task_id, affected_task_ids in failed_prerequisite_tasks.items():
            if task_id in affected_task_ids:
                return failed_task_id
        return None
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    prerequisite_task = CampusDatasetItem(
        task_id="prerequisite_task",
        task_type="email_sending",
        pre_task_for="dependent_task_1,dependent_task_2"
    )
    
    dependent_task_1 = CampusDatasetItem(
        task_id="dependent_task_1",
        task_type="email_sending"
    )
    
    dependent_task_2 = CampusDatasetItem(
        task_id="dependent_task_2",
        task_type="email_sending"
    )
    
    independent_task = CampusDatasetItem(
        task_id="independent_task",
        task_type="email_sending"
    )
    
    # æ¨¡æ‹Ÿå‰ç½®ä»»åŠ¡å¤±è´¥
    failed_session = Mock()
    failed_session.evaluation_record.outcome = SessionEvaluationOutcome.INCORRECT
    
    mock_record_failed_prerequisite_task(failed_session, prerequisite_task)
    
    # éªŒè¯å¤±è´¥è®°å½•
    assert "prerequisite_task" in failed_prerequisite_tasks
    assert "dependent_task_1" in failed_prerequisite_tasks["prerequisite_task"]
    assert "dependent_task_2" in failed_prerequisite_tasks["prerequisite_task"]
    print("âœ… å‰ç½®ä»»åŠ¡å¤±è´¥è®°å½•æˆåŠŸ")
    
    # æµ‹è¯•å—å½±å“ä»»åŠ¡æ£€æŸ¥
    assert mock_check_affected_by_failed_prerequisite("dependent_task_1") == "prerequisite_task"
    assert mock_check_affected_by_failed_prerequisite("dependent_task_2") == "prerequisite_task"
    assert mock_check_affected_by_failed_prerequisite("independent_task") is None
    print("âœ… å—å½±å“ä»»åŠ¡æ£€æŸ¥æ­£ç¡®")


def test_evaluation_logic():
    """æµ‹è¯•è¯„ä¼°é€»è¾‘"""
    print("\nğŸ“‹ æµ‹è¯•4: è¯„ä¼°é€»è¾‘æ¨¡æ‹Ÿ")
    
    # æ¨¡æ‹Ÿå®Œæ•´çš„è¯„ä¼°æµç¨‹
    failed_prerequisite_tasks = {"prerequisite_task": ["dependent_task_1", "dependent_task_2"]}
    
    def mock_complete_evaluation(task_id):
        """æ¨¡æ‹Ÿ _complete æ–¹æ³•ä¸­çš„é€»è¾‘"""
        # æ£€æŸ¥æ˜¯å¦å—å‰ç½®ä»»åŠ¡å¤±è´¥å½±å“
        for failed_task_id, affected_task_ids in failed_prerequisite_tasks.items():
            if task_id in affected_task_ids:
                session = Mock()
                session.evaluation_record.outcome = SessionEvaluationOutcome.INCORRECT
                session.task_output = {"result": "Set to incorrect due to the pre task"}
                session.evaluation_record.detail_dict = {
                    "failed_due_to_prerequisite": True,
                    "failed_prerequisite_task_id": failed_task_id,
                    "reason": f"Task failed because prerequisite task '{failed_task_id}' failed"
                }
                return session
        
        # æ­£å¸¸è¯„ä¼°ï¼ˆè¿™é‡Œç®€åŒ–ä¸ºæˆåŠŸï¼‰
        session = Mock()
        session.evaluation_record.outcome = SessionEvaluationOutcome.CORRECT
        session.task_output = {"result": "Task completed successfully"}
        return session
    
    # æµ‹è¯•å—å½±å“çš„ä»»åŠ¡
    result1 = mock_complete_evaluation("dependent_task_1")
    assert result1.evaluation_record.outcome == SessionEvaluationOutcome.INCORRECT
    assert result1.task_output["result"] == "Set to incorrect due to the pre task"
    assert result1.evaluation_record.detail_dict["failed_prerequisite_task_id"] == "prerequisite_task"
    print("âœ… dependent_task_1 æ­£ç¡®æ ‡è®°ä¸ºå› å‰ç½®ä»»åŠ¡å¤±è´¥è€Œå¤±è´¥")
    
    # æµ‹è¯•ä¸å—å½±å“çš„ä»»åŠ¡
    result2 = mock_complete_evaluation("independent_task")
    assert result2.evaluation_record.outcome == SessionEvaluationOutcome.CORRECT
    print("âœ… independent_task æ­£å¸¸è¯„ä¼°ä¸ºæˆåŠŸ")


def test_edge_cases():
    """æµ‹è¯•è¾¹ç•Œæƒ…å†µ"""
    print("\nğŸ“‹ æµ‹è¯•5: è¾¹ç•Œæƒ…å†µ")
    
    # ç©ºçš„ pre_task_for
    item1 = CampusDatasetItem(task_id="test1", task_type="email_sending", pre_task_for="")
    item2 = CampusDatasetItem(task_id="test2", task_type="email_sending", pre_task_for="   ")
    item3 = CampusDatasetItem(task_id="test3", task_type="email_sending")  # None
    
    # æ¨¡æ‹Ÿå¤„ç†é€»è¾‘
    def should_record_failure(item):
        return (item.pre_task_for is not None and item.pre_task_for.strip() != "")
    
    assert not should_record_failure(item1)  # ç©ºå­—ç¬¦ä¸²ä¸åº”è®°å½•
    assert not should_record_failure(item2)  # åªæœ‰ç©ºæ ¼ä¸åº”è®°å½•
    assert not should_record_failure(item3)  # Noneä¸åº”è®°å½•
    print("âœ… è¾¹ç•Œæƒ…å†µå¤„ç†æ­£ç¡®")
    
    # å¤æ‚çš„ä¾èµ–å­—ç¬¦ä¸²
    complex_item = CampusDatasetItem(
        task_id="complex_task",
        task_type="email_sending",
        pre_task_for="task1, , task2,  task3 ,,"
    )
    
    affected_tasks = [task_id.strip() for task_id in complex_item.pre_task_for.split(",") if task_id.strip()]
    assert affected_tasks == ["task1", "task2", "task3"]
    print("âœ… å¤æ‚ä¾èµ–å­—ç¬¦ä¸²è§£ææ­£ç¡®")


if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ§ª pre_task_for åŠŸèƒ½ç®€åŒ–æµ‹è¯•è„šæœ¬")
    print("=" * 60)
    
    try:
        test_campus_dataset_item_pre_task_for()
        test_pre_task_for_parsing()
        test_failed_prerequisite_tracking()
        test_evaluation_logic()
        test_edge_cases()
        
        print("\n" + "=" * 60)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼pre_task_for åŠŸèƒ½é€»è¾‘æ­£ç¡®")
        print("=" * 60)
        print("\nğŸ“ å…³äºä¾èµ–æ ‡è¯†ç¬¦çš„è¯´æ˜ï¼š")
        print("   åœ¨JSONæ•°æ®ä¸­ï¼š")
        print("   {")
        print('     "0_first_task": {          // â† JSON key (sample_index)')
        print('       "task_id": "first_task", // â† å®é™…ä»»åŠ¡ID')
        print('       "pre_task_for": "task2,task3", // â† åº”ä½¿ç”¨task_idå€¼')
        print("       // ...")
        print("     }")
        print("   }")
        print("   âœ… æ­£ç¡®åšæ³•ï¼šä½¿ç”¨ task_id å­—æ®µçš„å€¼ä½œä¸ºä¾èµ–æ ‡è¯†")
        print("   âŒ é”™è¯¯åšæ³•ï¼šä½¿ç”¨ JSON key ä½œä¸ºä¾èµ–æ ‡è¯†")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

